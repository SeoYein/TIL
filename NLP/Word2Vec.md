[Appendix]
1.	참고
-	희소 표현 https://soobarkbar.tistory.com/3
-	워드 임베딩 https://inforience.net/2019/10/05/word-embedding-for-fun/
-	Encoding 의미 https://inforience.net/2019/10/05/word-embedding-for-fun/
-	NNLM https://wikidocs.net/45609
-	RNNLM https://wikidocs.net/46496
-	언어 모델 https://wikidocs.net/21668

[Contents]
1.	배경 지식 
1.1	단어를 어떻게 컴퓨터가 인식하게 할까.
단어가 자연어 상태 그대로로는 컴퓨터에서 인식이 어렵다. 그래서 수(벡터)로 바꾸어주는 과정이 필요하다. 단어를 컴퓨터가 이해할 수 있게 나타내는 표현의 종류로는 크게 두 가지를 볼 수 있다. 
1.1.1	희소 표현(sparse representation)과 one-hot vector
희소 벡터를 이용해 단어를 표현하는 방법이다. 희소 벡터란 벡터 값의 대부분이 0으로 표현되어 있는 벡터이다. One-hot vector([1,0,0,0,…,0])의 경우가 희소 벡터의 형태를 띈다.
이러한 희소 표현으로 단어를 표현할 경우 문제점이 있다. 단어 벡터 두 개의 내적의 값이 항상 0이 된다. 이는 두 벡터가 항상 직교하기 때문에 단어 간의 유의성, 반의성과 상관 없이 단어 간 관계가 항상 같게 나타나게 된다. 
One-hot vector의 또 다른 문제점은, 문서 안의 단어의 수 만큼 벡터의 차원이 커진다는 것이다. 단어의 수가 10000개일 경우 [1,0,…0]부터 [0,0,…,1]까지 10000개의 단어 벡터를 생성해야 한다. 

1.1.2	밀집 표현(dense representation) 
밀집 벡터를 이용해 단어를 표현하는 방법이다. 밀집 벡터란, 희소 벡터가 1,0으로 구성되어 있는 것에 반해, 연속형의 실수 값으로 구성된 벡터이다. 특정한 값을 설정하면 그 값으로 벡터 표현의 차원을 맞춘다. Dense vector 요소 값은 연속형의 실수 값을 가진다. Sparse vector를 Dense vector로 바꾸는 과정은 학습을 통해 이루어진다. 
	
1.2	워드 임베딩 (word embedding)
단어를 밀집 벡터(dense vector)로 표현하는 방법을 말한다. embedding이란, 표현하고자 하는 대상을 벡터공간의 좌표로 매핑하여 표현하는 과정을 말한다. 
단어를 나타내는 dense vector를 embedding vector라고 한다. Word2Vec은 워드 임베딩을 하는 방식 중 하나이다. 
(+필요) 임베딩의 장점은 단어 간의 유사도 측정이 용이하다는 것이다. 

1.3	인코딩(encoding) 
사전적 의미로는 부호화하다, 라는 뜻. 데이터와 관련한 의미로는 입력 데이터를 학습 등을 위한 형태로 변환하는 것. 

1.4	언어 모델  
단어 시퀀스에 확률을 할당하는 걸 수행하는 모델. 어떤 단어 시퀀스가 가장 자연스러운지를 찾아내는 역할을 한다. 확률을 할당하는 방법 예시로는 이전 단어들이 주어졌을 때 다음 단어를 예측하는 것이 있다. 
언어 모델링은 주어진 단어로부터 알고자 하는 단어를 예측하는 작업을 말한다. 

2.	Word2Vec 이전 방법 
2.1	N_Gram
확률 기반의 모델. 언어 모델링 바로 앞의 n-1개의 단어만 참고하는 방법이다. 4-gram 언어 모델일 경우, 바로 앞의 3개 단어만 참고한다. 
P(w, boy is spreading) = count(boy is spreading w)/count(boy is spreading)
이런 식에서 훈련 코퍼스에서 boy is spreading 다음 단어가 등장할 확률을 예측한다.  
하지만 n-gram 언어 모델은 데이터가 충분하지 못하면 언어를 정확히 모델링하지 못한다. 이를 희소 문제(sparsity problem)라 한다. 

2.2	NNLM (Feed Forward Neural Network Language Model)
인공신경망 모델을 사용하여 기계가 자연어를 학습하게 하는 방법으로 n-gram이 가지고 있었던 희소 문제를 해결했다. 
NNLM도 앞의 모든 단어가 아닌 정해진 n개의 단어만을 참고한다. ‘what will the fat cat’이라는 단어 시퀀스가 있을 때, n=4일 경우 다음 단어 예측에 will the fat cat 이렇게 네 가지 단어만 참고한다. 이러한 참고 범위를 window라고 한다. 
단어들을 one-hot encoding하여 이 벡터들을 input이자 label로 사용한다. 
 
출처: https://wikidocs.net/45609
정답에 해당되는 단어 sit의 one-hot vector는 출력층에서 모델이 예측한 값의 오차를 구하기 위해 사용된다. 인공신경망의 학습은 이 오차로부터 손실함수를 사용해 이루어진다. 손실 함수는 cross-entropy 함수를 사용한다. 
(+추가 필요)
NNLM의 이전 방법(n-gram)과 비교했을 때의 장점은 수많은 문장에서
한계도 존재한다. N-gram 언어 모델과 마찬가지로 타겟 단어를 예측하기 위해, 정해진 n개의 단어만을 참고한다는 것. 버려지는 단어들이 지닌 정보를 활용할 수 없다. 

2.3	RNNLM (Recurrent Neural Network Language Model, RNNLM)
입력의 길이가 고정되어 있다는 NNLM의 한계를 극복하기 위해 RNN을 활용한 방법. 
RNNLM은 학습 시, 시퀀스(“will the fat cat sit”)를 입력으로 넣으면 타겟 시퀀스(“will the fat cat sit on”)을 예측하도록 훈련한다. (t시점의 출력이 t+1) 시점의 입력으로 사용되는 RNN 모델을 훈련시킴- 교사 강요(teacher forcing) 기법) 훈련 과정에서 사용하는 활성화 함수는 소프트맥스 함수이다. 모델이 예측한 값과 실제 레이블과의 오차 계산을 위한 손실 함수는 크로스 엔트로피 함수이다. 

2.3.1	RNN (Recurrent Neural Network) 
입력과 출력을 sequence 단위로 처리하는 모델. (시퀀스 모델) 
보통의 신경망은 활성화 함수를 지나는 값이 향하는 방향이 출력층 방향으로만 향했으나(feed-forward neural network), RNN은 그렇지 않은 신경망 중 하나. RNN은 활성화 함수를 거쳐 나온 값을 출력층, 은닉층 노드 다음 계산의 input으로도 보낸다. x를 입력층, y를 출력층이라고 보면 다음과 같다. 
  
출처 : https://wikidocs.net/22886
 
			출처 : https://wikidocs.net/46496
	

3.	Word2Vec
3.1	Word2Vec이란
단어를 벡터화할 때, 희소 벡터가 아닌 밀집 벡터로 벡터화하는 방법 중 하나. NNLM과 RNNLM에 비해 향상된 속도를 가지고 있다. 

3.2	Word2Vec 방식
3.2.1	CBOW
주변에 있는 단어를 가지고 중간에 있는 단어를 예측하는 방법을 통해 one-hot encoding된 단어를 차원이 줄어든 밀집 벡터로 (실수 값으로 이루어짐) 나타내게 된다. Word2vec은 딥러닝 모델은 아니다. 딥러닝은 여러 개의(혹은 수많은) 은닉층이 존재하는데 비해, word2vec은 학습 때 한 개의 은닉층만을 이용한다. 
“ the fat cat sat on the mat”을 예시 문장으로 하면, the, fat, cat, on, the, mat을 이용해서 sat을 예측하는 방식이 CBOW 방식이다. 예측하고자 하는 단어를 중심 단어(center word)라고 하고, 예측을 위해 활용하는 단어를 주변 단어 (context word)라고 한다. 중심 단어 예측을 위해 활용하는 단어의 수를 window라고 한다. CBOW에서 window 크기가 2이면 fat, cat, on, the 앞 뒤 2개씩 단어를 참고하게 된다. 
Sliding window : Window 크기가 정해지면, window를 계속 움직여 가며 주변 단어와 중심 단어를 다르게 하며 학습을 위한 데이터셋을 생성한다. 
 
출처 : https://wikidocs.net/22660

특징적인 점은, 앞선 인공신경망을 이용한 방법인 NNLM과 RNNML에 비해, word2vec에서는 활성화 함수를 사용하지 않는다. 대신 룩업 테이블이라는 연산을 담당하는 층인 투사층(projection layer)이 존재한다. 

 
출처 : https://wikidocs.net/22660

CBOW에서 투사층의 크기 M은 임베딩 벡터의 차원이 된다. 위 그림을 보면, CBOW를 통해 얻는 밀집 벡터인 임베딩 벡터는 차원이 5가 된다. 입력층과 투사층 사이의 가중치 W는 (단어 집합의 크기)*M의 행렬이다. 투사층에서 출력층 사이의 가중치 W’은 M*(단어 집합의 크기) 행렬이다. CBOW는 중심 단어(타겟 단어)를 더 정확히 맞추기 위해 W와 W’를 학습하는 과정을 가진다. 입력 벡터가 해당 단어의 인덱스에 1값을 갖고 나머지는 0인 one-hot 벡터이므로, 입력 벡터와 W의 곱은 W행렬의 해당 인덱스를 그대로 가져오는 것과 같다. 이 과정을 룩업 테이블(lookup table)이라고 한다.
CBOW에서는 주변 단어들을 투사한 벡터들을 투사층에서 평균을 구한다. 구해진 평균 벡터는 두번째 가중치 행렬 W’와 곱해지고, 곱셈의 결과로 입력 벡터와 차원이 같은 벡터가 나온다. 이 벡터에 softmax 함수를 넣어 값이 0과 1 사이의 실수이고 원소의 총 합이 1이 되게 한다. 이 스코어 벡터는 구하고자 하는 중심 단어의 one-hot vector와 가까워져야 한다. 중심 단어를 y이고 예측한 단어를 y hat이라 했을 때, CBOW에서는 손실 함수로 cross-entropy 함수를 사용한다. 
 
출처 : https://wikidocs.net/22660

역전파를 수행해서 크로스 엔트로피 값을 최소화하도록 학습한다. 학습이 된 후에는 W와 W’ 중 하나를 임베딩 벡터로 선택하거나, W와 W’의 평균치를 가지고 임베딩 벡터를 선택하기도 한다. 

3.2.2	Skip-Gram
Skip-Gram은 방식이 CBOW랑 비슷하지만 중심 단어를 가지고 주변 단어를 예측하는 방법이라는 차이점이 있다. 
중심 단어에서 주변 단어들을 예측하기 때문에, 투사층에서 벡터들의 평균을 구하는 과정이 없다. (투사 벡터가 여러 개가 아니므로) 
 
출처 : https://wikidocs.net/22660

3.2.3	계층적 소프트맥스(Hierarchical softmax)

3.2.4	SGNS(Skip-Gram with Negative Sampling) 
Skip-Gram(중심 단어로 주변 단어를 예측하는 방식)에서 추가적으로 Negative Sampling을 활용하는 방법 

3.4.1 Negative Sampling
Word2vec에서는 투사층에서 나온 값을 소프트맥스를 거쳐 0~1 사이의 값이며 합이 1로 되는 값으로 만들고 오차를 구하고, 오차를 줄이는 방식으로 임베딩을 조정한다. 이러한 작업은 중심 단어, 주변 단어와 상관이 없는 단어여도 진행이 된다. 그래서 단어 집합의 크기가 늘어나면 작업이 무거워지게 된다. 
그렇기에 ‘모든’단어 집합에 대해서 소프트맥스 함수와 역전파를 수행하는 게 아니라, 연관 관계(반의어, 유의어)가 있는 단어들에 한해 워드 임베딩을 조정하면 어떨까 하는 아이디어에서 나오게 된 게 negative sampling이다. 

3.3	Word2Vec 과 NNLM의 차이 
NNLM과 RNNML은 다음 단어를 예측하는 언어 모델로서의 기능이 목적이지만, Word2Vec은 워드 임베딩 자체가 목적이기 때문에 다음 단어가 아닌 중심 단어(타겟 단어)를 예측한다. CBOW 방식의 경우 주변 단어를 통해 중심 단어를 예측하게 하기 때문에, NNML이 이전의 지정된 n개의 단어만 참고하던 것과는 달리 Word2Vec은 sliding window를 이용해 예측 단어의 전, 후 단어들을 모두 참고하게 된다.
Word2Vec은 NNLM에 존재하던 활성화 함수가 있는 은닉층을 제거했다. 투사층 다음에 바로 출력층으로 연결된다. 그 외에 계층적 소프트맥스와 네거티브 샘플링을 이용해 연산량이 줄어들어 학습 속도가 증가하게 된다. 

4.	Word2Vec 이후의 방법
4.1	Glove
4.2	Fasttext 


